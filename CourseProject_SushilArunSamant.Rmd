---
title: "Practical Machine Learning Course Project Report"
author: "Sushil Arun Samant"
date: "June 12, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Motivation for doing the course

* Year 2018 was the year when for the first time the data generated by machines exceeded the human generated data in the world. 
* Now after the ongoing COVID-19 pandemic our lifes will no longer be the same, we are making a transition into a new world where data driven machine learning models will be our navigators for the journey into the remarkable wonderful future. 


### Course Project Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement, a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). The data for this project come from the same website. 
 
The training dataset :

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The testing dataset:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. 

### Systematic approach towards final goal

* Loading the data and necessary libraries 
* Data cleaning 
* Creating training and validation data sets
* Exploratory analysis using training data visualization
* Building various models using different metods on training set
* Prediciting the outputs for the validation data set using different models
* Compairing diferent models for their prediction accurucy on validation data set
* Choosing the most acurate model for predicting the 20 test cases 

### Loading the Data and necessary libraries
```{r }
library(ggplot2);library(caret)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
dim(training);dim(testing)
head(training)
```

### Cleaning the data set
Glimpse of the training data using head shows that column 1 to 7 are kind of meta data columns which can be removed. Also the data has lot of NA rows we need to fix these.
```{r }
training <- training[,-c(1:7)] 
testing<- testing[,-c(1:7)]
training<- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]

```
It will be also useful to remove variables which have near zero varience.
```{r }
inNzv <- nearZeroVar(training)
training <- training[,-inNzv]
dim(training); dim(testing)
```

### Creating training and validation sets
Lets divide the training set in to 70% training and 30% validation set. Then we will use the training set for all our exploratory analysis and building different models. Finally we will choose the best model by its performance on validation set and use it for predicting the test dataset.
```{r }
inTrain <- createDataPartition(y=training$classe, p=0.7, list=FALSE)
trainingSet <- training[inTrain,]
validationSet <- training[-inTrain,]
```

### Exploratory analysis using data visualization
Now lets  plot some of the input data columns against each other with colour given by the 'classe' column to understand little more about trends and patterns

```{r}
qplot(roll_belt,pitch_belt,col=classe, data=trainingSet)
qplot(yaw_belt, total_accel_belt,col=classe, data=trainingSet)
```

Most of the exploratory analysis done for this data set (only two are shown) shows small groups formations but hard to see any real trend or variable importance as number of variables are quite lage and the relationship they have with the 'classe' variable is complicated. So we will proceed ahead with building model using all the columns  other than 'classe' column as features.

### Building machine learning models
We will build two models for the trainingSet data using following algorithms in caret package

* rpart (Recursive Partitioning and Regression Trees)
* rf   (Classification and Regression with Random Forest)

We will also use 5 fold cross-validation technique

### Using rpart

```{r }
set.seed(12345)
trainingControl <- trainControl(method="cv", number=5, verboseIter=F)

rpartModel <- train(classe~., data=trainingSet, method="rpart", trControl = trainingControl)

rpartPred <- predict(rpartModel, validationSet)
confusionMatrix(rpartPred, factor(validationSet$classe))

```
### Using rf
```{r }
rfModel <- train(classe~., data=trainingSet, method="rf", trControl = trainingControl)

rfPred <- predict(rfModel, validationSet)
confusionMatrix(rfPred, factor(validationSet$classe))
```
Clearly the random forest model is the best one with validation set accuracy close to 99% meaning the expected out of sample error will be less than 1%, so we will use this model for predicting the test data set.


### Predictions for the test data set

```{r }
testingPred <- predict(rfModel,testing)
testingPred
```
